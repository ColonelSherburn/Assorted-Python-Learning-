import tensorflow as tf 
#from tensorflow.examples.tutorials.mnist import input_data
from create_sentiment_featuresets import create_feature_sets_and_labels
import time
import numpy as np

train_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')
'''
input > weight > hidden layer 1 (activation function) > weights > hidden layer 2 (activation function) > weights > output
feed forward 

compare output to intended output > cost function (cross entropy)

optimization function (optimizer) > minimize cost
backpropagation

feed forward + backprop = epoch

'''
# data set
#mnist = input_data.read_data_sets("/tmp/data", one_hot = True) # one element is on

n_nodes_hl1 = 500 # hidden layer first
n_nodes_hl2 = 500 # hidden layer second
n_nodes_hl3 = 500 # hidden layer third

n_classes = 2
batch_size = 100
                                                        # height x width
x = tf.placeholder('float', [None, len(train_x[0])])  # input data, height is none and width is 28 x 28 flattened
y = tf.placeholder('float')

def neural_network_model(data):
        # (input data * weights) + bias
        hidden_layer_1 = {'weights':tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])),
                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}

        hidden_layer_2 = {'weights' : tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),
                                  'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}


        hidden_layer_3 = {'weights' : tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),
                                  'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}


        output_layer = {'weights' : tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),
                                        'biases': tf.Variable(tf.random_normal([n_classes]))}


        # (input * weight) + biases

        l1 = tf.add(tf.matmul(data,hidden_layer_1['weights']), hidden_layer_1['biases'])
        l1 = tf.nn.relu(l1) # activation function

        l2 = tf.add(tf.matmul(l1, hidden_layer_2['weights']) , hidden_layer_2['biases'])
        l3 = tf.nn.relu(l2)  # activation function

        l3 = tf.add(tf.matmul(l2, hidden_layer_3['weights']) , hidden_layer_3['biases'])
        l3 = tf.nn.relu(l3)

        output = tf.add(tf.matmul(l3, output_layer['weights']) , output_layer['biases'])

        return output

def train_neural_network(x):
	# generate the mathematical map
	prediction = neural_network_model(x) # prediction from neural network, feed forward
	cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits = prediction,labels = y) )
	# difference between prediction and label generated by nn

	optimizer = tf.train.AdamOptimizer().minimize(cost)   # optimizer function, backpropagation
	# Adam Optimizer has a default learning rate of 0.001

	hm_epochs = 10   # number of cycles of feed forward + backpropagation

	with tf.Session() as sess:
		sess.run(tf.global_variables_initializer()) # run the session now that the mathematical graph has been described

		for epoch in range(hm_epochs):
			epoch_loss = 0 

			i = 0
			while i < len(train_x):
				start = i
				end = i + batch_size

				batch_x = np.array(train_x[start:end])
				batch_y = np.array(train_y[start:end])
				_, c = sess.run([optimizer, cost], feed_dict = {x: batch_x , y: batch_y})
				epoch_loss += c

				i += batch_size

			print('Epoch', epoch + 1, 'completed out of', hm_epochs, 'and loss:', epoch_loss)

		correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y,1))

		accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
		print('Accuracy:' , accuracy.eval({x:test_x, y:test_y}))

start_time = time.time()
train_neural_network(x)
print('Elapsed Time: %0.2f sec.' %(time.time()-start_time))